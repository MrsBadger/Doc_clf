{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Данные\n",
    "\n",
    "Данные в [архиве](https://drive.google.com/file/d/15o7fdxTgndoy6K-e7g8g1M2-bOOwqZPl/view?usp=sharing). В нём два файла:\n",
    "- `news_train.txt` тренировочное множество\n",
    "- `news_test.txt` тренировочное множество\n",
    "\n",
    "С некоторых новостных сайтов были загружены тексты новостей за период  несколько лет, причем каждая новость принаделжит к какой-то рубрике: `science`, `style`, `culture`, `life`, `economics`, `business`, `travel`, `forces`, `media`, `sport`.\n",
    "\n",
    "В каждой строке файла содержится метка рубрики, заголовок новостной статьи и сам текст статьи, например:\n",
    "\n",
    ">    **sport**&nbsp;&lt;tab&gt;&nbsp;**Сборная Канады по хоккею разгромила чехов**&nbsp;&lt;tab&gt;&nbsp;**Сборная Канады по хоккею крупно об...**"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Задача\n",
    "\n",
    "1. Обработать данные, получив для каждого текста набор токенов\n",
    "Обработать токены с помощью (один вариант из трех):\n",
    "    - pymorphy2\n",
    "    - русского [snowball стеммера](https://www.nltk.org/howto/stem.html)\n",
    "    - [SentencePiece](https://github.com/google/sentencepiece) или [Huggingface Tokenizers](https://github.com/huggingface/tokenizers)\n",
    "    \n",
    "    \n",
    "2. Обучить word embeddings (fastText, word2vec, gloVe) на тренировочных данных. Можно использовать [gensim](https://radimrehurek.com/gensim/models/word2vec.html) . Продемонстрировать семантические ассоциации. \n",
    "\n",
    "3. Реализовать алгоритм классификации, посчитать точноть на тестовых данных, подобрать гиперпараметры. Метод векторизации выбрать произвольно - можно использовать $tf-idf$ с понижением размерности (см. scikit-learn), можно использовать обученные на предыдущем шаге векторные представления, можно использовать [предобученные модели](https://rusvectores.org/ru/models/). Имейте ввиду, что простое \"усреднение\" токенов в тексте скорее всего не даст положительных результатов. Нужно реализовать два алгоритмов из трех:\n",
    "     - SVM\n",
    "     - наивный байесовский классификатор\n",
    "     - логистическая регрессия\n",
    "    \n",
    "\n",
    "4.* Реализуйте классификацию с помощью нейросетевых моделей. Например [RuBERT](http://docs.deeppavlov.ai/en/master/features/models/bert.html) или [ELMo](https://rusvectores.org/ru/models/)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 78,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pymorphy2\n",
    "import nltk\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import re\n",
    "from gensim.models import Word2Vec\n",
    "from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.naive_bayes import MultinomialNB\n",
    "from sklearn.model_selection import RandomizedSearchCV\n",
    "from sklearn.metrics import accuracy_score, precision_score, recall_score, f1_score\n",
    "from sklearn.decomposition import TruncatedSVD\n",
    "import warnings\n",
    "warnings.filterwarnings(\"ignore\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 58,
   "metadata": {},
   "outputs": [],
   "source": [
    "lines = list(open('news_train.txt', 'r', encoding='utf-8'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 59,
   "metadata": {},
   "outputs": [],
   "source": [
    "import random\n",
    "random.shuffle(lines)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 60,
   "metadata": {},
   "outputs": [],
   "source": [
    "train = lines[:10000]\n",
    "test = lines[10000:]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 61,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Counter({'economics': 2080,\n",
       "         'life': 2033,\n",
       "         'science': 2156,\n",
       "         'media': 2111,\n",
       "         'travel': 289,\n",
       "         'culture': 2053,\n",
       "         'sport': 2215,\n",
       "         'forces': 1225,\n",
       "         'business': 554,\n",
       "         'style': 284})"
      ]
     },
     "execution_count": 61,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from collections import Counter\n",
    "\n",
    "Counter([line.split('\\t')[0] for line in lines[:15000]])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 62,
   "metadata": {},
   "outputs": [],
   "source": [
    "dict_train = {'labels': [], 'headings': [], 'texts': []}\n",
    "dict_test = {'labels': [], 'headings': [], 'texts': []}\n",
    "\n",
    "for line in train:\n",
    "    label, heading, text = line.split('\\t')\n",
    "    dict_train['labels'].append(label)\n",
    "    dict_train['headings'].append(re.findall(r'\\b\\w+\\b', heading.lower()))\n",
    "    dict_train['texts'].append(re.findall(r'\\b\\w+\\b', text.lower()))\n",
    "\n",
    "for line in test:\n",
    "    label, heading, text = line.split('\\t')\n",
    "    dict_test['labels'].append(label)\n",
    "    dict_test['headings'].append(re.findall(r'\\b\\w+\\b', heading.lower()))\n",
    "    dict_test['texts'].append(re.findall(r'\\b\\w+\\b', text.lower()))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 63,
   "metadata": {},
   "outputs": [],
   "source": [
    "morph = pymorphy2.MorphAnalyzer()\n",
    "\n",
    "dict_train['texts'] = [[morph.parse(word)[0].normal_form for word in text] for text in dict_train['texts']]\n",
    "dict_test['texts'] = [[morph.parse(word)[0].normal_form for word in text] for text in dict_test['texts']]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 68,
   "metadata": {},
   "outputs": [],
   "source": [
    "model = Word2Vec(sentences=dict_train['texts'], workers=4)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 69,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[('болезнь', 0.7818340063095093),\n",
       " ('девочка', 0.7690442800521851),\n",
       " ('женщина', 0.7455222606658936),\n",
       " ('родитель', 0.7219225168228149),\n",
       " ('собака', 0.7218487858772278),\n",
       " ('ребёнок', 0.712665319442749),\n",
       " ('мальчик', 0.6760157346725464),\n",
       " ('хорошо', 0.6539921164512634),\n",
       " ('кровь', 0.6531692743301392),\n",
       " ('глаз', 0.6516709327697754)]"
      ]
     },
     "execution_count": 69,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model.wv.most_similar(positive=['жизнь'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 70,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[('развод', 0.7706078290939331),\n",
       " ('гибель', 0.735265851020813),\n",
       " ('вскоре', 0.7341554164886475),\n",
       " ('убийство', 0.7227871417999268),\n",
       " ('нападение', 0.7217468619346619),\n",
       " ('брак', 0.7181789875030518),\n",
       " ('девочка', 0.7101303339004517),\n",
       " ('задержание', 0.6926693320274353),\n",
       " ('погибший', 0.6722390651702881),\n",
       " ('мальчик', 0.6630831956863403)]"
      ]
     },
     "execution_count": 70,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model.wv.most_similar(positive=['смерть'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 71,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_train = [' '.join(text) for text in dict_train['texts']]\n",
    "y_train = dict_train['labels']\n",
    "X_test = [' '.join(text) for text in dict_test['texts']]\n",
    "y_test = dict_test['labels']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 73,
   "metadata": {},
   "outputs": [],
   "source": [
    "tfidf = TfidfVectorizer()\n",
    "\n",
    "X_train_vec = tfidf.fit_transform(X_train)\n",
    "X_test_vec = tfidf.transform(X_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 79,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "RandomizedSearchCV(cv=None, error_score=nan,\n",
       "                   estimator=LogisticRegression(C=1.0, class_weight=None,\n",
       "                                                dual=False, fit_intercept=True,\n",
       "                                                intercept_scaling=1,\n",
       "                                                l1_ratio=None, max_iter=100,\n",
       "                                                multi_class='auto', n_jobs=None,\n",
       "                                                penalty='l2', random_state=None,\n",
       "                                                solver='lbfgs', tol=0.0001,\n",
       "                                                verbose=0, warm_start=False),\n",
       "                   iid='deprecated', n_iter=10, n_jobs=None,\n",
       "                   param_distributions={'...\n",
       "       0.45, 0.46, 0.47, 0.48, 0.49, 0.5 , 0.51, 0.52, 0.53, 0.54, 0.55,\n",
       "       0.56, 0.57, 0.58, 0.59, 0.6 , 0.61, 0.62, 0.63, 0.64, 0.65, 0.66,\n",
       "       0.67, 0.68, 0.69, 0.7 , 0.71, 0.72, 0.73, 0.74, 0.75, 0.76, 0.77,\n",
       "       0.78, 0.79, 0.8 , 0.81, 0.82, 0.83, 0.84, 0.85, 0.86, 0.87, 0.88,\n",
       "       0.89, 0.9 , 0.91, 0.92, 0.93, 0.94, 0.95, 0.96, 0.97, 0.98, 0.99,\n",
       "       1.  ])},\n",
       "                   pre_dispatch='2*n_jobs', random_state=None, refit=True,\n",
       "                   return_train_score=False, scoring=None, verbose=0)"
      ]
     },
     "execution_count": 79,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "RandCV_logreg = RandomizedSearchCV(LogisticRegression(), param_distributions={'C': np.arange(0.01, 1.01, 0.01)})\n",
    "RandCV_naivebayes = RandomizedSearchCV(MultinomialNB(), param_distributions={'alpha': np.arange(0.5, 1.51, 0.01)})\n",
    "\n",
    "RandCV_logreg.fit(X_train_vec, y_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 80,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Precision:  0.874984534463844\n",
      "Recall:  0.8532\n",
      "F1 score:  0.8602019571375445\n",
      "Accuracy:  0.8532\n"
     ]
    }
   ],
   "source": [
    "y_pred_lreg = RandCV_logreg.best_estimator_.predict(X_test_vec)\n",
    "print('Precision: ', precision_score(y_pred_lreg, y_test, average='weighted'))\n",
    "print('Recall: ', recall_score(y_pred_lreg, y_test, average='weighted'))\n",
    "print('F1 score: ', f1_score(y_pred_lreg, y_test, average='weighted'))\n",
    "print('Accuracy: ', accuracy_score(y_pred_lreg, y_test))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 81,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "RandomizedSearchCV(cv=None, error_score=nan,\n",
       "                   estimator=MultinomialNB(alpha=1.0, class_prior=None,\n",
       "                                           fit_prior=True),\n",
       "                   iid='deprecated', n_iter=10, n_jobs=None,\n",
       "                   param_distributions={'alpha': array([0.5 , 0.51, 0.52, 0.53, 0.54, 0.55, 0.56, 0.57, 0.58, 0.59, 0.6 ,\n",
       "       0.61, 0.62, 0.63, 0.64, 0.65, 0.66, 0.67, 0.68, 0.69, 0.7 , 0.71,\n",
       "       0.72, 0.73, 0.74, 0.75, 0.76, 0.77, 0.78, 0.79, 0.8 , 0.81, 0.82,\n",
       "       0.83...\n",
       "       0.94, 0.95, 0.96, 0.97, 0.98, 0.99, 1.  , 1.01, 1.02, 1.03, 1.04,\n",
       "       1.05, 1.06, 1.07, 1.08, 1.09, 1.1 , 1.11, 1.12, 1.13, 1.14, 1.15,\n",
       "       1.16, 1.17, 1.18, 1.19, 1.2 , 1.21, 1.22, 1.23, 1.24, 1.25, 1.26,\n",
       "       1.27, 1.28, 1.29, 1.3 , 1.31, 1.32, 1.33, 1.34, 1.35, 1.36, 1.37,\n",
       "       1.38, 1.39, 1.4 , 1.41, 1.42, 1.43, 1.44, 1.45, 1.46, 1.47, 1.48,\n",
       "       1.49, 1.5 ])},\n",
       "                   pre_dispatch='2*n_jobs', random_state=None, refit=True,\n",
       "                   return_train_score=False, scoring=None, verbose=0)"
      ]
     },
     "execution_count": 81,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "RandCV_naivebayes.fit(X_train_vec, y_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 82,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Precision:  0.8783956455637224\n",
      "Recall:  0.8\n",
      "F1 score:  0.8309893778025075\n",
      "Accuracy:  0.8\n"
     ]
    }
   ],
   "source": [
    "y_pred_nb = RandCV_naivebayes.predict(X_test_vec)\n",
    "print('Precision: ', precision_score(y_pred_nb, y_test, average='weighted'))\n",
    "print('Recall: ', recall_score(y_pred_nb, y_test, average='weighted'))\n",
    "print('F1 score: ', f1_score(y_pred_nb, y_test, average='weighted'))\n",
    "print('Accuracy: ', accuracy_score(y_pred_nb, y_test))"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
